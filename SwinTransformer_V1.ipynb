{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import rasterio\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Dropout, BatchNormalization, Activation, Conv2DTranspose, concatenate, UpSampling2D, Concatenate, LayerNormalization, MultiHeadAttention, Dense, Reshape, Multiply, Lambda\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, Callback\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import Layer, Input, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Layer, Conv2D, BatchNormalization, Activation, UpSampling2D, Add\n",
    "from tensorflow.keras.models import Sequential\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.utils import Sequence\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "from scipy.ndimage import rotate\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "from tensorflow.keras.metrics import Precision, Recall\n",
    "import tifffile\n",
    "import random\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from tensorflow.keras import layers, models\n",
    "from contextlib import redirect_stdout\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "# Suppress informational messages and warnings (but not errors)\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # 0 = all messages are logged, 1 = INFO messages are not printed, 2 = INFO and WARNING messages are not printed, 3 = INFO, WARNING, and ERROR messages are not printed\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')  # This suppresses TensorFlow INFO and WARNING messages in Python\n",
    "\n",
    "#데이터 설정\n",
    "MAX_PIXEL_VALUE = 65535 # 이미지 정규화를 위한 픽셀 최대값\n",
    "LEARNING_RATE_DECAY = 1\n",
    "\n",
    "# from tensorflow.keras.mixed_precision import set_global_policy\n",
    "# # 혼합 정밀도 정책을 'mixed_float16'으로 설정\n",
    "# set_global_policy('mixed_float32')\n",
    "# # 정책 확인\n",
    "# print('Current mixed precision policy:', tf.keras.mixed_precision.global_policy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------\n",
    "<br><br>\n",
    "\n",
    "> * ### Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 저장 이름\n",
    "save_name = 'V4-P'\n",
    "\n",
    "N_FILTERS = 20 # 필터수 지정\n",
    "N_CHANNELS = 10 # channel 지정\n",
    "EPOCHS = 20 # 훈련 epoch 지정\n",
    "BATCH_SIZE = 16 # batch size 지정\n",
    "IMAGE_SIZE = (256, 256) # 이미지 크기 지정\n",
    "MODEL_NAME = 'swinT' # 모델 이름\n",
    "INITIAL_EPOCH = 0 # 초기 epoch\n",
    "WORKERS = 64\n",
    "LEARNING_RATE_DECAY = 0.90\n",
    "START_LARNING_RATE = 0.01\n",
    "FINAL_SPARSITY = 0.2\n",
    "\n",
    "#---------new--------------\n",
    "AUGMENT = False\n",
    "#--------------------------\n",
    "\n",
    "LOSS = 'dice+bc'  # 'dice' , 'focal' , 'tversky' , 'binary_crossentropy' , 'dice+bc' -> 파라미터는 아래서 조정\n",
    "\n",
    "#---------new--------------\n",
    "# 모델 파라미터 정의\n",
    "input_shape = (256, 256, 10)\n",
    "initial_channels = 20\n",
    "dim = 32\n",
    "num_heads = 8\n",
    "THRESHOLD = 0.5 # 고정값 -> 7번 channel 임계값\n",
    "#--------------------------\n",
    "\n",
    "EARLY_STOP_PATIENCE = 5 # 조기종료\n",
    "CHECKPOINT_PERIOD = 5 # 중간 가중치 저장 이름\n",
    "\n",
    "# 난수 시드 고정\n",
    "SEED = 10203\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "random.seed(SEED)\n",
    "os.environ['PYTHONHASHSEED']=str(SEED)\n",
    "\n",
    "\n",
    "# 사용할 데이터의 meta정보 가져오기\n",
    "train_meta = pd.read_csv('/home/jskim/aispark/AIspark_dataset/train_meta.csv')\n",
    "test_meta = pd.read_csv('/home/jskim/aispark/AIspark_dataset/test_meta.csv')\n",
    "# 데이터 위치\n",
    "IMAGES_PATH = '/home/jskim/aispark/AIspark_dataset/train_img'\n",
    "MASKS_PATH = '/home/jskim/aispark/AIspark_dataset/train_mask'\n",
    "# 가중치 저장 위치\n",
    "OUTPUT_DIR = '/home/jskim/aispark/train_output/'\n",
    "\n",
    "CHECKPOINT_MODEL_NAME = 'checkpoint-{}-{}-epoch_{{epoch:02d}}.hdf5'.format(MODEL_NAME, save_name)\n",
    "\n",
    "# 최종 가중치 저장 이름\n",
    "FINAL_WEIGHTS_OUTPUT = 'model_{}_{}_final_weights.h5'.format(MODEL_NAME, save_name)\n",
    "\n",
    "# 기본 로그 디렉토리 설정\n",
    "LOG_DIR = './logs/'\n",
    "\n",
    "# 로그 파일 이름을 현재 시간으로 설정\n",
    "log_filename = os.path.join(LOG_DIR, datetime.now().strftime('%Y-%m-%d_%H-%M-%S') + '_' + save_name + '_' + MODEL_NAME + '.log')\n",
    "\n",
    "# 로깅 기본 설정\n",
    "logging.basicConfig(filename=log_filename, level=logging.INFO, format='%(asctime)s:%(levelname)s:%(message)s')\n",
    "\n",
    "\n",
    "# 저장 폴더 없으면 생성\n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.makedirs(OUTPUT_DIR)\n",
    "\n",
    "# 사용할 GPU 이름\n",
    "CUDA_DEVICE = 1\n",
    "# GPU 설정\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(CUDA_DEVICE)\n",
    "try:\n",
    "    config = tf.compat.v1.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    sess = tf.compat.v1.Session(config=config)\n",
    "    K.set_session(sess)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    np.random.bit_generator = np.random._bit_generator\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------\n",
    "<br><br>\n",
    "\n",
    "> * ### Encoder structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nstage1 : (None, 64, 64, 20)\\nstage2 : (None, 32, 32, 40)\\nstage3 : (None, 16, 16, 80)\\nstage4 : (None, 8, 8, 160)\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class EMSABlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, dim, num_heads, channel):\n",
    "        super(EMSABlock, self).__init__()\n",
    "        self.dim = dim\n",
    "        self.num_heads = num_heads\n",
    "        self.channel = channel  # 입력 텐서의 채널 수\n",
    "        self.depthwise_conv = layers.DepthwiseConv2D(kernel_size=1, padding='same')\n",
    "        self.layer_norm = layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.qkv = layers.Dense(dim * 3, use_bias=False)\n",
    "        self.reshape = layers.Reshape((-1, num_heads, dim // num_heads))\n",
    "        self.transpose = layers.Permute((2, 1, 3))\n",
    "        \n",
    "        self.fc = layers.Dense(self.channel)  # 여기서 self.channel은 입력의 채널 수와 동일해야 합니다.\n",
    "\n",
    "\n",
    "        self.mlp = models.Sequential([\n",
    "            layers.Dense(dim * 4, activation='relu'),\n",
    "            layers.Dense(self.channel),\n",
    "        ])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Depthwise Convolution and Layer Norm\n",
    "        x = self.depthwise_conv(inputs)\n",
    "        x = self.layer_norm(x)\n",
    "        \n",
    "        # Generating q, k, v\n",
    "        qkv = self.qkv(x)\n",
    "        q, k, v = tf.split(qkv, 3, axis=-1)\n",
    "        q = self.transpose(self.reshape(q))\n",
    "        k = self.transpose(self.reshape(k))\n",
    "        v = self.transpose(self.reshape(v))\n",
    "        \n",
    "        # Attention with MatMul\n",
    "        attn_output = tf.matmul(q, k, transpose_b=True)\n",
    "        attn_output = tf.nn.softmax(attn_output)\n",
    "        attn_output = tf.matmul(attn_output, v)\n",
    "\n",
    "        # attn_output을 원래 입력 형태로 다시 reshape 합니다.\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        seq_len = tf.shape(inputs)[1]\n",
    "        channel = tf.shape(inputs)[3]\n",
    "        attn_output = tf.reshape(attn_output, [batch_size, seq_len, seq_len, channel])\n",
    "        \n",
    "        # Skip connection\n",
    "        attn_output = self.fc(attn_output) + inputs\n",
    "\n",
    "        # MLP with Skip connection\n",
    "        mlp_output = self.mlp(attn_output) + attn_output\n",
    "        \n",
    "        return mlp_output\n",
    "\n",
    "def create_stem_layer(input_shape):\n",
    "    model = Sequential()\n",
    "    \n",
    "    # 첫 번째 Conv2D와 MaxPooling2D를 통해 H/2, W/2의 축소를 수행\n",
    "    model.add(Conv2D(initial_channels/2, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "    \n",
    "    # 두 번째 Conv2D와 MaxPooling2D를 통해 H/4, W/4의 축소를 수행\n",
    "    model.add(Conv2D(initial_channels, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "    \n",
    "    return model\n",
    "\n",
    "class PositionalEmbedding2D(tf.keras.layers.Layer):\n",
    "    def __init__(self, scale=0.1):\n",
    "        super(PositionalEmbedding2D, self).__init__()\n",
    "        self.scale = scale\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # `input_shape` 파라미터는 입력 텐서의 형태를 나타냅니다.\n",
    "        # 이 형태에 기반하여 올바른 차원의 포지셔널 임베딩을 생성합니다.\n",
    "        height, width, channels = input_shape[1], input_shape[2], input_shape[3]\n",
    "        self.pos_emb = self.add_weight(\n",
    "            name=\"pos_emb\", \n",
    "            shape=(1, height, width, channels), \n",
    "            initializer=\"random_normal\"  # 초기화 방식을 'random_normal'로 변경\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # 입력 텐서에 포지셔널 임베딩을 더해 반환합니다.\n",
    "        return inputs + self.scale * self.pos_emb\n",
    "\n",
    "def create_downsampling_and_positional_embedding(input_shape, filters):\n",
    "    model = Sequential()\n",
    "    # Downsampling\n",
    "    model.add(Conv2D(filters, kernel_size=(3, 3), strides=(2, 2), padding='same', activation='relu'))\n",
    "    # Positional Embedding. 여기서는 scale 매개변수만을 사용하면 됩니다.\n",
    "    # 입력 형태는 build 또는 call 메소드가 호출될 때 자동으로 처리됩니다.\n",
    "    model.add(PositionalEmbedding2D(scale=0.1))\n",
    "    return model\n",
    "\n",
    "def create_e_transformer_block(channels, num_heads):\n",
    "    # Adjusted to pass `channels`\n",
    "    e_transformer_block = models.Sequential()\n",
    "    e_transformer_block.add(EMSABlock(dim, num_heads, channels))\n",
    "    e_transformer_block.add(EMSABlock(dim, num_heads, channels))\n",
    "    return e_transformer_block\n",
    "\n",
    "\n",
    "'''\n",
    "stage1 : (None, 64, 64, 20)\n",
    "stage2 : (None, 32, 32, 40)\n",
    "stage3 : (None, 16, 16, 80)\n",
    "stage4 : (None, 8, 8, 160)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------\n",
    "<br><br>\n",
    "\n",
    "> * ### Decoder structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MLPBlock(filters, input_tensor):\n",
    "    # Define an MLP block to be used in the decoder after each fusion\n",
    "    mlp = models.Sequential([\n",
    "        layers.Dense(filters, use_bias=False),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Activation('gelu'),\n",
    "        layers.Dense(filters, use_bias=False),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Activation('gelu')\n",
    "    ], name='MLPBlock')\n",
    "    return mlp(input_tensor)\n",
    "\n",
    "def FlattenMLPReshape(filters, input_tensor):\n",
    "    # Flatten, process through an MLP block and reshape for upsampling\n",
    "    flattened = layers.Flatten()(input_tensor)\n",
    "    mlp_output = MLPBlock(filters, flattened)\n",
    "    # Reshape back to a 4D tensor. Note that the shape will be inferred.\n",
    "    reshaped = layers.Reshape((input_tensor.shape[1] * 2, input_tensor.shape[2] * 2, filters))(mlp_output)\n",
    "    return reshaped\n",
    "\n",
    "class PyramidPoolingModule(Layer):\n",
    "    def __init__(self, bin_sizes, **kwargs):\n",
    "        super(PyramidPoolingModule, self).__init__(**kwargs)\n",
    "        self.bin_sizes = bin_sizes\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.convs = []\n",
    "        for bin_size in self.bin_sizes:\n",
    "            self.convs.append(Sequential([\n",
    "                Conv2D(input_shape[-1], kernel_size=(1, 1), strides=(1, 1), padding='same'),\n",
    "                BatchNormalization(),\n",
    "                Activation('relu')\n",
    "            ]))\n",
    "\n",
    "    def call(self, inputs):\n",
    "        concat_list = [inputs]\n",
    "        h = inputs.shape[1]\n",
    "        w = inputs.shape[2]\n",
    "        for bin_size, conv in zip(self.bin_sizes, self.convs):\n",
    "            x = tf.keras.layers.AveragePooling2D(pool_size=(h // bin_size, w // bin_size))(inputs)\n",
    "            x = conv(x)\n",
    "            x = UpSampling2D(size=(h // x.shape[1], w // x.shape[2]))(x)\n",
    "            concat_list.append(x)\n",
    "        return Concatenate()(concat_list)\n",
    "\n",
    "def create_decoder(C1, C2, C3, C4):\n",
    "    # PPM을 사용한 후 채널 수 조정을 위해 1x1 컨볼루션 레이어를 적용\n",
    "    ppm = PyramidPoolingModule(bin_sizes=[1, 2, 3, 6])(C4)\n",
    "    ppm = Conv2D(C3.shape[3], kernel_size=(1, 1), padding='same')(ppm)  # C3와 채널 수 일치\n",
    "    upsampled_ppm = UpSampling2D(size=(2, 2))(ppm)\n",
    "    \n",
    "    # C3와 결합\n",
    "    P3 = Add()([upsampled_ppm, C3])\n",
    "    upsampled_P3 = UpSampling2D(size=(2, 2))(P3)\n",
    "    compressed_channels3 = Conv2D(filters=C2.shape[3], kernel_size=(1, 1), padding='same')(upsampled_P3)\n",
    "    \n",
    "    # C2와 결합하기 전에 C2의 채널 수를 맞춥니다.\n",
    "    C2_adjusted = Conv2D(C2.shape[3], kernel_size=(1, 1), padding='same')(C2)\n",
    "    P2 = Add()([compressed_channels3, C2_adjusted])\n",
    "    upsampled_P2 = UpSampling2D(size=(2, 2))(P2)\n",
    "    compressed_channels2 = Conv2D(filters=C1.shape[3], kernel_size=(1, 1), padding='same')(upsampled_P2)\n",
    "\n",
    "    # C1과 결합하기 전에 C1의 채널 수를 맞춥니다.\n",
    "    C1_adjusted = Conv2D(C1.shape[3], kernel_size=(1, 1), padding='same')(C1)\n",
    "    P1 = Add()([compressed_channels2, C1_adjusted])\n",
    "\n",
    "    # 컨볼루션을 적용하여 최종 세그멘테이션 맵 생성\n",
    "    segmentation_map = Conv2D(filters=1, kernel_size=(1, 1), activation='sigmoid')(P1)\n",
    "\n",
    "    return segmentation_map\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------\n",
    "<br><br>\n",
    "\n",
    "> * ### Edge Enhancement and Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def edge_enhancement(original_image, segmentation_result):\n",
    "    # 원본 이미지에서 7번째 채널을 선택합니다.\n",
    "    channel_7 = original_image[..., 6:7]  # 0-indexed, 7번째 채널은 인덱스 6입니다.\n",
    "    # 임계값 이상의 픽셀을 선별하고, 나머지 픽셀을 0으로 설정합니다.\n",
    "    highlighted_pixels = tf.where(channel_7 >= THRESHOLD, channel_7, 0)\n",
    "\n",
    "    # 선별된 픽셀 값을 최대 픽셀 값으로 확장합니다.\n",
    "    # 선별된 픽셀들에 대해 (픽셀 값 - 임계값)을 계산하여 범위를 [0, MAX_PIXEL_VALUE]로 매핑합니다.\n",
    "    highlighted_pixels_scaled = tf.where(highlighted_pixels > 0, ((highlighted_pixels - THRESHOLD) / (MAX_PIXEL_VALUE - THRESHOLD)) * MAX_PIXEL_VALUE, 0)\n",
    "\n",
    "    # 엣지 추출\n",
    "    edge_features = Conv2D(1, (4, 4), padding='same', activation='sigmoid')(highlighted_pixels_scaled)\n",
    "\n",
    "    # 이진화\n",
    "    def binarize(x):\n",
    "        return K.cast(K.greater(x, 0.5), K.floatx())  # 임계값을 0.5로 설정\n",
    "\n",
    "    binarized_edges = Lambda(binarize)(edge_features)\n",
    "\n",
    "    # 정규화\n",
    "    normalized_edges = BatchNormalization()(binarized_edges)\n",
    "\n",
    "    # 정규화된 엣지 맵을 세그멘테이션 결과에 더합니다.\n",
    "    segmentation_result_upsampled = UpSampling2D(size=(4, 4))(segmentation_result)  # 64x64에서 256x256으로 업샘플링\n",
    "    enhanced_segmentation = Add()([segmentation_result_upsampled, normalized_edges])\n",
    "\n",
    "    return enhanced_segmentation\n",
    "\n",
    "def create_final_model(input_shape, initial_channels, dim, num_heads):\n",
    "    input_tensor = Input(shape=input_shape)\n",
    "    \n",
    "    # 스템 레이어 및 포지셔널 임베딩 적용\n",
    "    x = create_stem_layer(input_shape)(input_tensor)  # 수정된 부분: input_shape 인자 추가\n",
    "    x = PositionalEmbedding2D()(x)\n",
    "\n",
    "    # 첫 번째 E-Transformer 블록 및 다운샘플링\n",
    "    stage1_output = create_e_transformer_block(initial_channels, num_heads)(x)\n",
    "    stage1_output = create_e_transformer_block(initial_channels, num_heads)(stage1_output)\n",
    "    x = create_downsampling_and_positional_embedding(input_shape, filters=initial_channels*2)(stage1_output)\n",
    "\n",
    "    # 두 번째 E-Transformer 블록 및 다운샘플링\n",
    "    stage2_output = create_e_transformer_block(initial_channels*2, num_heads)(x)\n",
    "    stage2_output = create_e_transformer_block(initial_channels*2, num_heads)(stage2_output)\n",
    "    x = create_downsampling_and_positional_embedding(input_shape, filters=initial_channels*4)(stage2_output)\n",
    "\n",
    "    # 세 번째 E-Transformer 블록 및 다운샘플링\n",
    "    stage3_output = create_e_transformer_block(initial_channels*4, num_heads)(x)\n",
    "    stage3_output = create_e_transformer_block(initial_channels*4, num_heads)(stage3_output)\n",
    "    x = create_downsampling_and_positional_embedding(input_shape, filters=initial_channels*8)(stage3_output)\n",
    "\n",
    "    # 네 번째 E-Transformer 블록\n",
    "    stage4_output = create_e_transformer_block(initial_channels*8, num_heads)(x)\n",
    "    stage4_output = create_e_transformer_block(initial_channels*8, num_heads)(stage4_output)\n",
    "    \n",
    "    # Decoder 및 Edge Enhancement (디코더 및 엣지 강화 구현 필요)\n",
    "    segmentation_result = create_decoder(stage1_output, stage2_output, stage3_output, stage4_output)\n",
    "    enhanced_segmentation_output = edge_enhancement(input_tensor, segmentation_result)\n",
    "    \n",
    "    # 최종 모델\n",
    "    final_model = Model(inputs=input_tensor, outputs=enhanced_segmentation_output)\n",
    "    return final_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------\n",
    "<br><br>\n",
    "\n",
    "> * ### dataset , metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(Sequence):\n",
    "    def __init__(self, img_paths, mask_paths, batch_size=32, default_path=None, augment=False, add_noise=False, noise_factor=0.01, shuffle=True):\n",
    "        self.img_paths = img_paths\n",
    "        self.mask_paths = mask_paths\n",
    "        self.batch_size = batch_size\n",
    "        self.default_path = default_path if default_path else \"\"\n",
    "        self.augment = augment\n",
    "        self.add_noise = add_noise\n",
    "        self.noise_factor = noise_factor\n",
    "        self.shuffle = shuffle\n",
    "\n",
    "        if self.shuffle:\n",
    "            self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths) // self.batch_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_img_paths = self.img_paths[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        batch_mask_paths = self.mask_paths[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "\n",
    "        batch_imgs = []\n",
    "        batch_masks = []\n",
    "\n",
    "        for img_path, mask_path in zip(batch_img_paths, batch_mask_paths):\n",
    "            img = tifffile.imread(os.path.join(self.default_path, img_path))\n",
    "            mask = tifffile.imread(os.path.join(self.default_path, mask_path))\n",
    "\n",
    "            # 이미지 전처리\n",
    "            if self.augment:\n",
    "                img, mask = self.augment_image(img, mask)\n",
    "\n",
    "            img = img.astype(np.float32) / MAX_PIXEL_VALUE\n",
    "            mask = mask.astype(np.float32)\n",
    "            mask = np.expand_dims(mask, axis=-1)\n",
    "\n",
    "            batch_imgs.append(img)\n",
    "            batch_masks.append(mask)\n",
    "        return np.array(batch_imgs), np.array(batch_masks)\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            indices = np.arange(len(self.img_paths))\n",
    "            np.random.shuffle(indices)\n",
    "            self.img_paths = np.array(self.img_paths)[indices].tolist()\n",
    "            self.mask_paths = np.array(self.mask_paths)[indices].tolist()\n",
    "\n",
    "    def augment_image(self, img, mask):\n",
    "        decision_flip_lr = tf.random.uniform([], seed=SEED) < 0.25\n",
    "        decision_flip_ud = tf.random.uniform([], seed=SEED) > 0.75\n",
    "        decision_noise = tf.random.uniform([], seed=SEED) < 0.1\n",
    "        decision_crop_resize = tf.random.uniform([], seed=SEED) < 0.25  # 크롭 및 리사이즈 결정을 위한 랜덤 조건\n",
    "        original_size = (self.original_height, self.original_width)\n",
    "\n",
    "        if decision_crop_resize:\n",
    "            combined = tf.concat([img, mask], axis=-1)\n",
    "            combined = tf.image.random_crop(combined, size=[self.target_height, self.target_width, tf.shape(combined)[-1]], seed=SEED)\n",
    "            img, mask = combined[..., :-1], combined[..., -1:]\n",
    "            img = tf.image.resize(img, original_size)\n",
    "            mask = tf.image.resize(mask, original_size, method='nearest')\n",
    "\n",
    "        if decision_flip_lr:\n",
    "            img = np.flip(img, axis=1)\n",
    "            mask = np.flip(mask, axis=1)\n",
    "\n",
    "        if decision_flip_ud:\n",
    "            img = np.flip(img, axis=0)\n",
    "            mask = np.flip(mask, axis=0)\n",
    "\n",
    "        # 노이즈 추가 부분도 동일한 방식으로 랜덤 결정을 시드와 함께 사용\n",
    "        if self.add_noise and decision_noise:\n",
    "            img = self.add_gaussian_noise(img, self.noise_factor)\n",
    "\n",
    "        return img, mask\n",
    "\n",
    "    \n",
    "    def shuffle_lists(self, images_path, masks_path, random_state=None):\n",
    "        # random_state가 주어진 경우, 난수 생성기의 시드를 설정합니다.\n",
    "        if random_state is not None:\n",
    "            random.seed(random_state)\n",
    "        \n",
    "        # images_path와 masks_path 리스트를 함께 섞기 위해 두 리스트의 쌍을 만듭니다.\n",
    "        paired_lists = list(zip(images_path, masks_path))\n",
    "        random.shuffle(paired_lists)\n",
    "        \n",
    "        # 섞인 리스트를 다시 분리합니다.\n",
    "        shuffled_images_path, shuffled_masks_path = zip(*paired_lists)\n",
    "        \n",
    "        # zip() 함수는 튜플을 반환하므로, 리스트로 변환해 반환합니다.\n",
    "        return list(shuffled_images_path), list(shuffled_masks_path)\n",
    "\n",
    "\n",
    "    def add_gaussian_noise(self, image, mean=0, std=0.01, noise_factor=0.1):\n",
    "        \"\"\"\n",
    "        이미지에 가우시안 노이즈를 추가하는 함수.\n",
    "        mean: 노이즈의 평균값\n",
    "        std: 노이즈의 표준편차\n",
    "        noise_factor: 노이즈의 전체적인 강도를 조절하는 계수\n",
    "        \"\"\"\n",
    "        gaussian_noise = np.random.normal(mean, std, image.shape) * noise_factor * MAX_PIXEL_VALUE\n",
    "        noisy_image = image + gaussian_noise\n",
    "        noisy_image = np.clip(noisy_image, 0, MAX_PIXEL_VALUE)  # 픽셀 값을 0과 MAX_PIXEL_VALUE 사이로 제한\n",
    "        return noisy_image\n",
    "    \n",
    "def lr_decay(epoch, lr):\n",
    "    \"\"\"\n",
    "    에포크에 따라 학습률을 감소시키는 함수.\n",
    "    매 epoch마다 학습률을 0.95배 해주는 예시.\n",
    "    \"\"\"\n",
    "    new_lr = lr * LEARNING_RATE_DECAY\n",
    "    logging.info(f\"Learning rate : {new_lr}\")\n",
    "    return new_lr\n",
    "\n",
    "class F1Score(tf.keras.metrics.Metric):\n",
    "    def __init__(self, name='f1_score', **kwargs):\n",
    "        super(F1Score, self).__init__(name=name, **kwargs)\n",
    "        self.precision = tf.keras.metrics.Precision()\n",
    "        self.recall = tf.keras.metrics.Recall()\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        self.precision.update_state(y_true, y_pred, sample_weight)\n",
    "        self.recall.update_state(y_true, y_pred, sample_weight)\n",
    "\n",
    "    def result(self):\n",
    "        p = self.precision.result()\n",
    "        r = self.recall.result()\n",
    "        return 2 * ((p * r) / (p + r + tf.keras.backend.epsilon()))\n",
    "\n",
    "    def reset_states(self):\n",
    "        self.precision.reset_states()\n",
    "        self.recall.reset_states()\n",
    "\n",
    "class IoU(tf.keras.metrics.Metric):\n",
    "    def __init__(self, name='iou', **kwargs):\n",
    "        super(IoU, self).__init__(name=name, **kwargs)\n",
    "        self.intersection = self.add_weight(name=\"intersection\", initializer=\"zeros\")\n",
    "        self.union = self.add_weight(name=\"union\", initializer=\"zeros\")\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_pred = tf.round(y_pred)  # 세그멘테이션 마스크를 이진 형태로 변환\n",
    "        intersection = tf.reduce_sum(y_true * y_pred)\n",
    "        union = tf.reduce_sum(y_true) + tf.reduce_sum(y_pred) - intersection\n",
    "        \n",
    "        # 이전 상태에 누적\n",
    "        self.intersection.assign_add(intersection)\n",
    "        self.union.assign_add(union)\n",
    "\n",
    "    def result(self):\n",
    "        return self.intersection / (self.union + tf.keras.backend.epsilon())\n",
    "\n",
    "    def reset_states(self):\n",
    "        # 에포크가 끝날 때마다 상태 초기화\n",
    "        self.intersection.assign(0)\n",
    "        self.union.assign(0)\n",
    "\n",
    "class LoggingCallback(Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # IoU 값을 로그에 포함시키기 위해 'iou'와 'val_iou' 키를 로그에서 검색\n",
    "        # 키가 없는 경우 None을 반환하기 위해 get 메소드 사용\n",
    "        iou = logs.get('iou')\n",
    "        val_iou = logs.get('val_iou')\n",
    "        # IoU 값이 제공되는 경우 로그 메시지에 포함, 그렇지 않은 경우 \"N/A\"로 표시\n",
    "        logging.info(f\"Epoch {epoch + 1}, Loss: {logs['loss']}, Accuracy: {logs['accuracy']}, Val_Loss: {logs['val_loss']}, Val_Accuracy: {logs['val_accuracy']}, IoU: {iou if iou is not None else 'N/A'}, Val_IoU: {val_iou if val_iou is not None else 'N/A'}\")\n",
    "\n",
    "def dice_loss_function(y_true, y_pred):\n",
    "    smooth = 1e-6\n",
    "    y_true_f = tf.reshape(y_true, [-1])\n",
    "    y_pred_f = tf.reshape(y_pred, [-1])\n",
    "    intersection = tf.reduce_sum(y_true_f * y_pred_f)\n",
    "    return 1 - (2. * intersection + smooth) / (tf.reduce_sum(y_true_f) + tf.reduce_sum(y_pred_f) + smooth)\n",
    "\n",
    "def dice_coef(y_true, y_pred):\n",
    "    numerator = 2 * tf.reduce_sum(y_true * y_pred)\n",
    "    denominator = tf.reduce_sum(y_true + y_pred)\n",
    "    return (numerator + 1) / (denominator + 1)\n",
    "\n",
    "def combined_dice_crossentropy_loss(y_true, y_pred):\n",
    "    dice_loss = dice_loss_function(y_true, y_pred)  # 앞서 정의한 dice_loss_function 사용\n",
    "    cross_entropy_loss = tf.keras.losses.binary_crossentropy(y_true, y_pred)\n",
    "    return dice_loss + cross_entropy_loss\n",
    "\n",
    "def focal_loss(gamma=2., alpha=.25):\n",
    "    def focal_loss_fixed(y_true, y_pred):\n",
    "        pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n",
    "        pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n",
    "        return -K.sum(alpha * K.pow(1. - pt_1, gamma) * K.log(pt_1)) - K.sum((1-alpha) * K.pow(pt_0, gamma) * K.log(1. - pt_0))\n",
    "    return focal_loss_fixed\n",
    "\n",
    "def tversky_loss(beta):\n",
    "    def loss(y_true, y_pred):\n",
    "        numerator = K.sum(y_true * y_pred, axis=-1)\n",
    "        denominator = y_true * y_pred + beta * (1 - y_true) * y_pred + (1 - beta) * y_true * (1 - y_pred)\n",
    "        return 1 - (numerator + 1) / (K.sum(denominator, axis=-1) + 1)\n",
    "    return loss\n",
    "\n",
    "# 두 샘플 간의 유사성 metric\n",
    "def dice_coef(y_true, y_pred, smooth=1):\n",
    "    intersection = K.sum(y_true * y_pred, axis=[1,2,3])\n",
    "    union = K.sum(y_true, axis=[1,2,3]) + K.sum(y_pred, axis=[1,2,3])\n",
    "    dice = K.mean((2. * intersection + smooth)/(union + smooth), axis=0)\n",
    "    return dice\n",
    "\n",
    "# 이미지 경로에서 이미지를 읽어와 정규화한 후 numpy 배열로 반환\n",
    "def get_img_arr(path):\n",
    "    img = rasterio.open(path).read().transpose((1, 2, 0))\n",
    "    img = np.float32(img)/MAX_PIXEL_VALUE\n",
    "\n",
    "    return img\n",
    "\n",
    "# 마스크 이미지 경로에서 이미지를 읽어와 numpy 배열로 반환\n",
    "def get_mask_arr(path):\n",
    "    img = rasterio.open(path).read().transpose((1, 2, 0))\n",
    "    seg = np.float32(img)\n",
    "    return seg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------\n",
    "<br><br>\n",
    "\n",
    "> * ### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26860 6715\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "#------------------------new------------------------\n",
    "ADD_NOISE = False\n",
    "NOISE_FACTOR=0.1\n",
    "LOSS=combined_dice_crossentropy_loss,\n",
    "METRICS = [IoU(), 'accuracy', F1Score(), dice_coef]\n",
    "#---------------------------------------------------\n",
    "\n",
    "\n",
    "# 변수 로그에 기록\n",
    "logging.info(f'Save name: {save_name}')\n",
    "logging.info(f'Number of filters: {N_FILTERS}')\n",
    "logging.info(f'Number of channels: {N_CHANNELS}')\n",
    "logging.info(f'Epochs: {EPOCHS}')\n",
    "logging.info(f'Batch size: {BATCH_SIZE}')\n",
    "logging.info(f'Image size: {IMAGE_SIZE}')\n",
    "logging.info(f'Model name: {MODEL_NAME}')\n",
    "logging.info(f'Initial epoch: {INITIAL_EPOCH}')\n",
    "logging.info(f'Workers: {WORKERS}')\n",
    "logging.info(f'Learning rate decay: {LEARNING_RATE_DECAY}')\n",
    "logging.info(f'Start learning rate: {START_LARNING_RATE}')\n",
    "logging.info(f'Seed: {SEED}')\n",
    "\n",
    "# train : val = 8 : 2 나누기\n",
    "x_tr, x_val = train_test_split(train_meta, test_size=0.2, random_state=SEED)\n",
    "print(len(x_tr), len(x_val))\n",
    "\n",
    "# train : val 지정 및 generator\n",
    "images_train = [os.path.join(IMAGES_PATH, image) for image in x_tr['train_img'] ]\n",
    "masks_train = [os.path.join(MASKS_PATH, mask) for mask in x_tr['train_mask'] ]\n",
    "\n",
    "images_validation = [os.path.join(IMAGES_PATH, image) for image in x_val['train_img'] ]\n",
    "masks_validation = [os.path.join(MASKS_PATH, mask) for mask in x_val['train_mask'] ]\n",
    "\n",
    "# 학습 데이터용 인스턴스 생성, 데이터 증강 적용\n",
    "train_generator = DataGenerator(images_train, masks_train, batch_size=BATCH_SIZE, default_path=IMAGES_PATH, augment=AUGMENT,add_noise=ADD_NOISE, noise_factor=NOISE_FACTOR)\n",
    "# 검증 데이터용 인스턴스 생성, 데이터 증강 미적용\n",
    "validation_generator = DataGenerator(images_validation, masks_validation, batch_size=BATCH_SIZE, default_path=IMAGES_PATH, augment=False, add_noise=False)\n",
    "\n",
    "# 최종 모델 생성\n",
    "final_model = create_final_model(input_shape, initial_channels, dim, num_heads)\n",
    "# 모델 컴파일\n",
    "final_model.compile(optimizer=Adam(learning_rate=START_LARNING_RATE),\n",
    "                    loss=LOSS,\n",
    "                    metrics=METRICS)\n",
    "\n",
    "# 콜백 정의\n",
    "es = EarlyStopping(monitor='val_iou', mode='max', verbose=1, patience=EARLY_STOP_PATIENCE)\n",
    "checkpoint = ModelCheckpoint(os.path.join(OUTPUT_DIR, CHECKPOINT_MODEL_NAME), \n",
    "                             monitor='val_iou',\n",
    "                             verbose=1, save_best_only=True, mode='max', save_freq='epoch')\n",
    "lr_scheduler = LearningRateScheduler(lr_decay)\n",
    "\n",
    "# 콜백에 Pruning 콜백 추가\n",
    "callbacks = [\n",
    "    checkpoint,\n",
    "    es,\n",
    "    lr_scheduler,\n",
    "    LoggingCallback()\n",
    "]\n",
    "\n",
    "# 모델 훈련\n",
    "history = final_model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=len(images_train) // BATCH_SIZE,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=len(images_validation) // BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=[checkpoint, es, lr_scheduler, LoggingCallback()],\n",
    "    workers=WORKERS,\n",
    "    initial_epoch=INITIAL_EPOCH\n",
    ")\n",
    "\n",
    "# 필요 시 final_model 저장\n",
    "logging.info('가중치 저장')\n",
    "model_weights_output = os.path.join(OUTPUT_DIR, FINAL_WEIGHTS_OUTPUT)\n",
    "final_model.save(model_weights_output)\n",
    "logging.info(f\"저장된 가중치 명: {model_weights_output}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------\n",
    "<br><br>\n",
    "\n",
    "> * ### Make Pickle , Submit format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n    optimizer = torch.optim.SGD([{'params':\\n                                      filter(lambda p: p.requires_grad,\\n                                             model.parameters()),\\n                                      'lr': args2.lr}],\\n                                    lr=args2.lr,\\n                                    momentum=0.9,\\n                                    weight_decay=0.0005,\\n                                    nesterov=False,\\n                                    )\\n\\n    optimizer = torch.optim.AdamW([{'params':\\n                                      filter(lambda p: p.requires_grad,\\n                                             model.parameters()),\\n                                  'lr': args2.lr}],\\n                                lr=args2.lr,\\n                                betas=(0.9, 0.999),\\n                                weight_decay=0.01,\\n                                )\\n\""
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 제출용 pkl 파일 만들기\n",
    "final_model = create_final_model(input_shape, initial_channels, dim, num_heads)\n",
    "final_model.compile(optimizer = Adam(), loss = LOSS, metrics = ['accuracy'])\n",
    "final_model.summary()\n",
    "\n",
    "final_model.load_weights('model_{}_{}_final_weights.h5'.format(MODEL_NAME, save_name))\n",
    "\n",
    "y_pred_dict = {}\n",
    "\n",
    "# 가정: test_meta는 테스트 이미지 파일명을 포함하는 딕셔너리나 리스트입니다.\n",
    "for i in test_meta['test_img']:  # tqdm은 진행 상태 바를 표시합니다.\n",
    "    img = get_img_arr(f'/home/jskim/aispark/AIspark_dataset/test_img/{i}')\n",
    "    y_pred = model.predict(np.array([img]), batch_size=1)\n",
    "\n",
    "    y_pred = np.where(y_pred[0, :, :, 0] > 0.25, 1, 0)  # 임계값 처리\n",
    "    y_pred = y_pred.astype(np.uint8)\n",
    "    y_pred_dict[i] = y_pred\n",
    "\n",
    "joblib.dump(y_pred_dict, '/home/jskim/aispark/y_pred_{}_{}.pkl'.format(MODEL_NAME, save_name))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------\n",
    "<br><br>\n",
    "\n",
    "> * ### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 특정 이미지에 대한 예측 마스크 시각화 함수\n",
    "def visualize_prediction(test_image_name):\n",
    "    # 테스트 이미지 로딩\n",
    "    img = get_img_arr(f'./AIspark_dataset/test_img/{test_image_name}')\n",
    "    \n",
    "    # 예측된 마스크 로딩\n",
    "    y_pred = y_pred_dict[test_image_name]\n",
    "    \n",
    "    # 이미지와 예측된 마스크 시각화\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    axs[0].imshow(img[:, :, 0], cmap='gray')  # 첫 번째 채널을 그레이스케일 이미지로 시각화\n",
    "    axs[0].set_title('Original Image')\n",
    "    axs[0].axis('off')\n",
    "\n",
    "    axs[1].imshow(y_pred, cmap='gray')  # 예측된 마스크 시각화\n",
    "    axs[1].set_title('Predicted Mask')\n",
    "    axs[1].axis('off')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def load_tif_image(image_path):\n",
    "    with rasterio.open(image_path) as img_file:\n",
    "        img = img_file.read()\n",
    "    return np.transpose(img, (1, 2, 0))\n",
    "\n",
    "def visualize_channel_images(image):\n",
    "    channels = image.shape[2]\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    for i in range(channels):\n",
    "        plt.subplot(2, (channels+1)//2, i+1)\n",
    "        plt.imshow(image[:, :, i])\n",
    "        plt.title(f'Channel {i+1}')\n",
    "        plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# 시각화\n",
    "# 예측 결과 딕셔너리 로드\n",
    "y_pred_dict = joblib.load('/home/jskim/aispark/y_pred_{}_{}.pkl'.format(MODEL_NAME, save_name))\n",
    "\n",
    "# 예시: 'test_img_2427.tif' 이미지에 대한 예측 마스크 시각화\n",
    "visualize_prediction('test_img_2423.tif')\n",
    "image_path = '/home/jskim/aispark/AIspark_dataset/test_img/test_img_2423.tif'\n",
    "visualize_channel_images(load_tif_image(image_path))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aispark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
